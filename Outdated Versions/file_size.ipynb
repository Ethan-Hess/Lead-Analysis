{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "folder_path = 'D:\\VSCode\\Lead Analysis'\n",
    "csv_files = glob.glob(os.path.join(folder_path, '**', '*.csv'), recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My counter without different encoding methods\n",
    "leads_breakdown = pd.DataFrame(columns=['Filename', '# of rows', 'Column List'])\n",
    "print(len(csv_files))\n",
    "for file in csv_files:\n",
    "    name = Path(file).name\n",
    "    print(name)\n",
    "    row_count = 0\n",
    "    columns_df = pd.read_csv(file, nrows=0)\n",
    "    columns = columns_df.columns\n",
    "    for chunk in pd.read_csv(file,chunksize=1000000 ,usecols=[0], on_bad_lines='skip'):\n",
    "        row_count += len(chunk)\n",
    "    column_list = ', '.join(columns)\n",
    "    new_row = pd.DataFrame([{'Filename': name, '# of rows': row_count, 'Column List': column_list}])\n",
    "    leads_breakdown = pd.concat([leads_breakdown, new_row], ignore_index=True)\n",
    "    print(f'\\nDone: {row_count}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame\n",
    "leads_breakdown = pd.DataFrame(columns=['Filename', '# of rows', 'Column List'])\n",
    "\n",
    "# List of possible encodings to try\n",
    "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "print(len(csv_files))\n",
    "for file in csv_files:\n",
    "    # Get the filename\n",
    "    name = Path(file).name\n",
    "    \n",
    "    print(name)\n",
    "    row_count = 0\n",
    "    columns = None\n",
    "    \n",
    "    # Try different encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            columns_df = pd.read_csv(file, nrows=0, encoding=encoding)\n",
    "            columns = columns_df.columns\n",
    "            break  # If it succeeds, no need to try other encodings\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Try the next encoding\n",
    "\n",
    "    if columns is None:\n",
    "        print(f\"Failed to decode {name} with available encodings.\")\n",
    "        continue\n",
    "    \n",
    "    # Count rows in chunks\n",
    "    for chunk in pd.read_csv(file, chunksize=1000000, usecols=[0], on_bad_lines='skip', encoding=encoding):\n",
    "        row_count += len(chunk)\n",
    "    \n",
    "    # Create new row with information\n",
    "    column_list = ', '.join(columns)\n",
    "    new_row = pd.DataFrame([{'Filename': name, '# of rows': row_count, 'Column List': column_list}])\n",
    "    leads_breakdown = pd.concat([leads_breakdown, new_row], ignore_index=True)\n",
    "    print(f'\\nDone: {row_count}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "keyword = 'plumb'\n",
    "\n",
    "chunk_division = range(15)\n",
    "temp_files = list(range(500))\n",
    "csv_chunk_size = 1000000\n",
    "filtered_chunks = []\n",
    "folder_path = 'D:\\VSCode\\Lead Analysis'\n",
    "csv_files = glob.glob(os.path.join(folder_path, '**', '*.csv'), recursive=True)\n",
    "\n",
    "def chunk_files(file_list): # Equally divides the list based off of chunk size\n",
    "    files_per_chunk = int(len(file_list) / len(chunk_division))\n",
    "    chunk = {}\n",
    "    x = 0\n",
    "    for i in chunk_division:\n",
    "        y = x + files_per_chunk if i < max(chunk_division) else len(file_list)\n",
    "        chunk[f'{i}'] = file_list[x:y]\n",
    "        x = y\n",
    "    return chunk\n",
    "\n",
    "def chunk_search(file_list:dict, chunk_start, chunk_end):\n",
    "    search_results = pd.DataFrame()\n",
    "    processing_list = {}\n",
    "\n",
    "    # inclusive range of file index\n",
    "    if chunk_start == chunk_end:\n",
    "        processing_list = file_list\n",
    "    else:\n",
    "        processing_range = range(chunk_start, chunk_end + 1)\n",
    "        for i in processing_range:\n",
    "            print(i)\n",
    "            processing_list[f'{i}'] = file_list[f'{i}']\n",
    "    print(file_list)\n",
    "    print(type(processing_list))\n",
    "\n",
    "\n",
    "    for index, files in processing_list.items():\n",
    "        for file in files:\n",
    "            print(f'Current File: {file}')\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    for chunk in pd.read_csv(file, chunksize=1000000, on_bad_lines='skip', encoding=encoding):\n",
    "                        result = chunk[chunk.apply(lambda row: row.astype(str).str.contains('plumb').any(), axis=1)]\n",
    "                        search_results = pd.concat([search_results, result], ignore_index=True)\n",
    "                    break  # If it succeeds, no need to try other encodings\n",
    "                except UnicodeDecodeError:\n",
    "                    continue  # Try the next encoding\n",
    "\n",
    "    search_results.to_csv('Output\\plumb_list.csv', index=False) #16.49 to search whole dataset\n",
    "\n",
    "greg = {}\n",
    "greg[1] = csv_files\n",
    "print(greg)\n",
    "chunk_search(greg, 1, 1) # inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_breakdown.to_csv('Output\\lead_analysis.csv', index=False)\n",
    "leads_breakdown.to_csv('Output\\lead_plumb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2063 prev set                                                                                                                            Click execute above cells ^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
